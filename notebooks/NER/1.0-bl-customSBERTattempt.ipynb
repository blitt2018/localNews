{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/opt/anaconda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/opt/anaconda/lib/python3.8/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/opt/anaconda/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch \n",
    "from transformers import DistilBertModel\n",
    "from transformers import DistilBertTokenizer\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#Build up to SBERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceNum = 1\n",
    "device = torch.device(\"cuda:\" + str(deviceNum) if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.000000GB\n",
      "torch.cuda.memory_reserved: 0.000000GB\n",
      "torch.cuda.memory_free: 0.000000GB\n",
      "torch.cuda.max_memory_reserved: 0.000000GB\n"
     ]
    }
   ],
   "source": [
    "def check_mem():\n",
    "    torch.cuda.empty_cache()\n",
    "    a = torch.cuda.memory_allocated(deviceNum)/1024/1024/1024\n",
    "    r = torch.cuda.memory_reserved(deviceNum)/1024/1024/1024\n",
    "    print(\"torch.cuda.memory_allocated: %fGB\"%a)\n",
    "    print(\"torch.cuda.memory_reserved: %fGB\"%r)\n",
    "    print(\"torch.cuda.memory_free: %fGB\"%(r-a))\n",
    "    print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(deviceNum)/1024/1024/1024))\n",
    "check_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken almost directly from: \n",
    "#https://www.pinecone.io/learn/train-sentence-transformers-softmax/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear up memory \n",
    "\n",
    "#torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1676, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4.0, 4.0, 1.0, 4.0, 4.0, 1.6666666666666667, ...</td>\n",
       "      <td>MARTINSBURG, W.Va. — A suspected drunken drive...</td>\n",
       "      <td>PORT-AU-PRINCE, Haiti — Haitian President Jove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4.0, 4.0, 1.0, 4.0, 3.6666666666666665, 1.666...</td>\n",
       "      <td>Share This On:\\n\\nPin 11 Shares\\n\\n(NEWS ROOM ...</td>\n",
       "      <td>BERLIN - A fire at a zoo in western Germany in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.0, 2.0, 1.0, 2.333333333333333, 2.333333333...</td>\n",
       "      <td>(Breitbart) – President Donald Trump welcomed ...</td>\n",
       "      <td>PALM BEACH, United States — US President Donal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0, 2.333333333333333, 2.6666666666666665, 1...</td>\n",
       "      <td>Uber has sold its online food-ordering busines...</td>\n",
       "      <td>Rapid digitisation and growth in both online b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.0, 1.25, 1.0, 1.25, 1.25, 1.0, 1.0]</td>\n",
       "      <td>BENGALURU (Reuters) - India has approved its t...</td>\n",
       "      <td>BANGALORE: India plans to make a fresh attempt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ground_truth  \\\n",
       "0  [4.0, 4.0, 1.0, 4.0, 4.0, 1.6666666666666667, ...   \n",
       "1  [4.0, 4.0, 1.0, 4.0, 3.6666666666666665, 1.666...   \n",
       "2  [1.0, 2.0, 1.0, 2.333333333333333, 2.333333333...   \n",
       "3  [1.0, 2.333333333333333, 2.6666666666666665, 1...   \n",
       "4             [1.0, 1.25, 1.0, 1.25, 1.25, 1.0, 1.0]   \n",
       "\n",
       "                                               text1  \\\n",
       "0  MARTINSBURG, W.Va. — A suspected drunken drive...   \n",
       "1  Share This On:\\n\\nPin 11 Shares\\n\\n(NEWS ROOM ...   \n",
       "2  (Breitbart) – President Donald Trump welcomed ...   \n",
       "3  Uber has sold its online food-ordering busines...   \n",
       "4  BENGALURU (Reuters) - India has approved its t...   \n",
       "\n",
       "                                               text2  \n",
       "0  PORT-AU-PRINCE, Haiti — Haitian President Jove...  \n",
       "1  BERLIN - A fire at a zoo in western Germany in...  \n",
       "2  PALM BEACH, United States — US President Donal...  \n",
       "3  Rapid digitisation and growth in both online b...  \n",
       "4  BANGALORE: India plans to make a fresh attempt...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data \n",
    "df = pd.read_csv(\"/shared/3/projects/benlitterer/localNews/NetworkMVP/enTrainData.csv\", sep=\"\\t\")\n",
    "df = df.loc[(df[\"url1_lang\"] == \"en\") & (df[\"url2_lang\"] == \"en\")]\n",
    "\n",
    "#put ground truth values into a list \n",
    "df[\"ground_truth\"] = df[[\"Geography\", 'Entities', 'Time', 'Narrative', 'Overall', 'Style', 'Tone']].values.tolist()\n",
    "\n",
    "#TODO: do we need to make data tensors or does this happen when we call set_format() later on\n",
    "#df[\"ground_truth\"] = df[\"ground_truth\"].apply(torch.tensor)\n",
    "\n",
    "#[torch.tensor(gtList) for gtList in df[\"ground_truth\"]]\n",
    "\n",
    "#get only the columns we need \n",
    "#TODO: do we need \"pair_id\"? \n",
    "leanDf = df[[\"ground_truth\",  'text1', 'text2']].dropna()\n",
    "\n",
    "print(leanDf.shape)\n",
    "leanDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4.0, 4.0, 1.0, 4.0, 4.0, 1.6666666666666667, ...</td>\n",
       "      <td>MARTINSBURG, W.Va. — A suspected drunken drive...</td>\n",
       "      <td>PORT-AU-PRINCE, Haiti — Haitian President Jove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4.0, 4.0, 1.0, 4.0, 3.6666666666666665, 1.666...</td>\n",
       "      <td>Share This On:\\n\\nPin 11 Shares\\n\\n(NEWS ROOM ...</td>\n",
       "      <td>BERLIN - A fire at a zoo in western Germany in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.0, 2.0, 1.0, 2.333333333333333, 2.333333333...</td>\n",
       "      <td>(Breitbart) – President Donald Trump welcomed ...</td>\n",
       "      <td>PALM BEACH, United States — US President Donal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0, 2.333333333333333, 2.6666666666666665, 1...</td>\n",
       "      <td>Uber has sold its online food-ordering busines...</td>\n",
       "      <td>Rapid digitisation and growth in both online b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.0, 1.25, 1.0, 1.25, 1.25, 1.0, 1.0]</td>\n",
       "      <td>BENGALURU (Reuters) - India has approved its t...</td>\n",
       "      <td>BANGALORE: India plans to make a fresh attempt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2871</th>\n",
       "      <td>[4.0, 3.0, 1.0, 2.0, 3.0, 1.0, 1.0]</td>\n",
       "      <td>Hello,\\n\\nWe are preparing to move our environ...</td>\n",
       "      <td>I'm running vCenter 6.7 w/ three host also run...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>[1.0, 4.0, 3.0, 4.0, 4.0, 2.0, 1.0]</td>\n",
       "      <td>You can now consult with your doctors through ...</td>\n",
       "      <td>India is doing a commendable job in fighting t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>[1.0, 2.0, 1.0, 2.0, 1.0, 3.0, 1.0]</td>\n",
       "      <td>Jakraphanth Thomma on Saturday killed his comm...</td>\n",
       "      <td>Medics carry a stretcher towards a Thai shoppi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>The bodies of the victims were found lying in ...</td>\n",
       "      <td>Johannesburg - Gauteng police on Saturday said...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875</th>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>CIUDAD HIDALGO, Mexico — Hundreds of Central A...</td>\n",
       "      <td>Hundreds of Central American migrants crossed ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1676 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ground_truth  \\\n",
       "0     [4.0, 4.0, 1.0, 4.0, 4.0, 1.6666666666666667, ...   \n",
       "1     [4.0, 4.0, 1.0, 4.0, 3.6666666666666665, 1.666...   \n",
       "2     [1.0, 2.0, 1.0, 2.333333333333333, 2.333333333...   \n",
       "3     [1.0, 2.333333333333333, 2.6666666666666665, 1...   \n",
       "4                [1.0, 1.25, 1.0, 1.25, 1.25, 1.0, 1.0]   \n",
       "...                                                 ...   \n",
       "2871                [4.0, 3.0, 1.0, 2.0, 3.0, 1.0, 1.0]   \n",
       "2872                [1.0, 4.0, 3.0, 4.0, 4.0, 2.0, 1.0]   \n",
       "2873                [1.0, 2.0, 1.0, 2.0, 1.0, 3.0, 1.0]   \n",
       "2874                [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "2875                [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]   \n",
       "\n",
       "                                                  text1  \\\n",
       "0     MARTINSBURG, W.Va. — A suspected drunken drive...   \n",
       "1     Share This On:\\n\\nPin 11 Shares\\n\\n(NEWS ROOM ...   \n",
       "2     (Breitbart) – President Donald Trump welcomed ...   \n",
       "3     Uber has sold its online food-ordering busines...   \n",
       "4     BENGALURU (Reuters) - India has approved its t...   \n",
       "...                                                 ...   \n",
       "2871  Hello,\\n\\nWe are preparing to move our environ...   \n",
       "2872  You can now consult with your doctors through ...   \n",
       "2873  Jakraphanth Thomma on Saturday killed his comm...   \n",
       "2874  The bodies of the victims were found lying in ...   \n",
       "2875  CIUDAD HIDALGO, Mexico — Hundreds of Central A...   \n",
       "\n",
       "                                                  text2  \n",
       "0     PORT-AU-PRINCE, Haiti — Haitian President Jove...  \n",
       "1     BERLIN - A fire at a zoo in western Germany in...  \n",
       "2     PALM BEACH, United States — US President Donal...  \n",
       "3     Rapid digitisation and growth in both online b...  \n",
       "4     BANGALORE: India plans to make a fresh attempt...  \n",
       "...                                                 ...  \n",
       "2871  I'm running vCenter 6.7 w/ three host also run...  \n",
       "2872  India is doing a commendable job in fighting t...  \n",
       "2873  Medics carry a stretcher towards a Thai shoppi...  \n",
       "2874  Johannesburg - Gauteng police on Saturday said...  \n",
       "2875  Hundreds of Central American migrants crossed ...  \n",
       "\n",
       "[1676 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leanDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PORT-AU-PRINCE, Haiti — Haitian President Jovenel Moïse broke with tradition on Wednesday and celebrated the country’s independence day in the capital for security reasons following months of political turmoil', '\\n\\nMoïse, whose government has been accused of corruption, denounced graft during his speech at the National Palace in Port-au-Prince and urged Haiti’s elite to work with the government and help create employment', '\\n\\n“We’re still extremely poor,” he said', ' “Those who continue to get rich find it normal that they do not pay taxes, find it normal that there can be no competition, find it normal that they set prices for consumers, especially when this consumer is the state itself', '”\\n\\nMoïse also apologized for the country’s ongoing power outages and renewed his 2016 campaign pledge to provide electricity 24 hours a day, saying it was harder to accomplish than he imagined', '\\n\\nThe speech that marked the 216th anniversary of the world’s first black republic was originally slated to take place in the northern coastal town of Gonaives, where Jean-Jacques Dessalines declared Haiti’s independence', ' But the town, like many others, was hit by violent protests that began in September amid anger over corruption, fuel shortages and dwindling food supplies as opposition leaders and supporters demanded the resignation of Moïse', ' More than 40 people have been killed and dozens injured', '\\n\\nLarge-scale protests in Port-au-Prince have since dissipated, although smaller ones are still occurring elsewhere in the country', ' On Wednesday, opposition leaders and supporters gathered in Gonaives to attend the funeral of an anti-government protester and then carried his coffin through the streets as more protesters joined them', '\\n\\nCopyright 2020 The Associated Press', ' All rights reserved', ' This material may not be published, broadcast, rewritten or redistributed', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def getTopSentences(inStr):\n",
    "    print(re.split(\"[.!]\", inStr))\n",
    "testStr = leanDf.loc[0,\"text2\"]\n",
    "#print(testStr)\n",
    "getTopSentences(testStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data loaded in properly \n",
    "dataset = Dataset.from_pandas(leanDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562d5d18e1be4e7f9dd71905820dd8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1676 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9667aa14cd34453b98c2d5ec37984429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1676 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "all_cols = [\"ground_truth\"]\n",
    "\n",
    "\"\"\"\n",
    "for part in ['text1', 'text2']:\n",
    "    dataset = dataset.map(\n",
    "        lambda x: tokenizer(\n",
    "            x[part], max_length=128, padding='max_length',\n",
    "            truncation=True\n",
    "        ), batched=True\n",
    "    )\n",
    "    \n",
    "    for col in ['input_ids', 'attention_mask']:\n",
    "        dataset = dataset.rename_column(\n",
    "            col, part+'_'+col\n",
    "        )\n",
    "        all_cols.append(part+'_'+col)\n",
    "print(all_cols)\n",
    "\"\"\"\n",
    "\n",
    "for part in [\"text1\", \"text2\"]: \n",
    "    #tokenizes each row of the dataset and gives us back tuple of lists \n",
    "    dataset=dataset.map(lambda x: tokenizer(x[\"text1\"], max_length=100, padding='max_length', truncation=True))\n",
    "    \n",
    "    for col in ['input_ids', 'attention_mask']: \n",
    "        dataset = dataset.rename_column(\n",
    "            col, part+'_'+col\n",
    "        )\n",
    "        all_cols.append(part+'_'+col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ground_truth', 'text1_input_ids', 'text1_attention_mask', 'text2_input_ids', 'text2_attention_mask']\n"
     ]
    }
   ],
   "source": [
    "#we have the correct input format needed for the text data \n",
    "print(all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataset features to PyTorch tensors\n",
    "dataset.set_format(type='torch', columns=all_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.000000GB\n",
      "torch.cuda.memory_reserved: 0.000000GB\n",
      "torch.cuda.memory_free: 0.000000GB\n",
      "torch.cuda.max_memory_reserved: 0.000000GB\n"
     ]
    }
   ],
   "source": [
    "check_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows: 1676\n",
      "train rows: 1257\n",
      "validation rows: 419\n",
      "sum: 1676\n"
     ]
    }
   ],
   "source": [
    "length = len(dataset)\n",
    "print(\"total rows: \" + str(length))\n",
    "trainProp = .75 \n",
    "\n",
    "validLen = int(length * (1-trainProp))\n",
    "trainLen = length - validLen\n",
    "print(\"train rows: \" + str(trainLen) + \"\\nvalidation rows: \" + str(validLen) + \"\\nsum: \" + str(trainLen + validLen))\n",
    "\n",
    "train, valid = torch.utils.data.random_split(dataset, [trainLen, validLen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7fc07ccbca60>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize the dataloader\n",
    "batch_size = 2\n",
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    train, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "validLoader = torch.utils.data.DataLoader(\n",
    "    valid, batch_size=1, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pool(token_embeds, attention_mask):\n",
    "    # reshape attention_mask to cover 768-dimension embeddings\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(\n",
    "        token_embeds.size()\n",
    "    ).float()\n",
    "    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
    "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n",
    "        in_mask.sum(1), min=1e-9\n",
    "    )\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.000000GB\n",
      "torch.cuda.memory_reserved: 0.000000GB\n",
      "torch.cuda.memory_free: 0.000000GB\n",
      "torch.cuda.max_memory_reserved: 0.000000GB\n"
     ]
    }
   ],
   "source": [
    "check_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# start from a pretrained bert-base-uncased model\n",
    "# TODO: consider something already trained for sentance similarity? \n",
    "#del model \n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "#model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.248280GB\n",
      "torch.cuda.memory_reserved: 0.265625GB\n",
      "torch.cuda.memory_free: 0.017345GB\n",
      "torch.cuda.max_memory_reserved: 0.265625GB\n"
     ]
    }
   ],
   "source": [
    "check_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the feed-forward NN first\n",
    "#this will go from a pooled input to our 6 prediction labels \n",
    "ffnn = torch.nn.Linear(768*3, 7).to(device)\n",
    "\n",
    "#TODO: double check on if reduction=\"mean\" is the right move here...\n",
    "#could cosine similarity also work..? I think that is between the two predicted vectors though.. \n",
    "loss_func = torch.nn.MSELoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we would initialize everything first\n",
    "optim = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "# and setup a warmup for the first ~10% steps\n",
    "total_steps = int(len(dataset) / batch_size)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=warmup_steps, num_training_steps=total_steps - warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7593948136311601, 0.9205664999205727, 0.8863416070394171]\n"
     ]
    }
   ],
   "source": [
    "dummyPreds = [[3, 1, 9], [8, 9, 10], [2, 4, 1], [10, 10, 9]]\n",
    "dummyGT = [[2, 2, 6], [12, 5, 8], [3, 3, 3], [7, 8, 9]]\n",
    "\n",
    "def getCorr(inPreds, inGT): \n",
    "    inPreds = np.array(inPreds)\n",
    "    inGT = np.array(inGT)\n",
    "    corrList = []\n",
    "    for col in range(inPreds.shape[1]): \n",
    "        corr = np.corrcoef(inPreds[:,col], inGT[:,col])[0][1]\n",
    "        corrList.append(corr)\n",
    "    return corrList\n",
    "\n",
    "print(getCorr(dummyPreds, dummyGT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.994450GB\n",
      "torch.cuda.memory_reserved: 1.087891GB\n",
      "torch.cuda.memory_free: 0.093441GB\n",
      "torch.cuda.max_memory_reserved: 1.457031GB\n"
     ]
    }
   ],
   "source": [
    "check_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b8b908b24e4f24987aaaab1d0c1b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/629 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 1; 10.92 GiB total capacity; 7.16 GiB already allocated; 4.38 MiB free; 7.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-9212e3fc396b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0;31m# extract token embeddings from BERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                     u = model(\n\u001b[0m\u001b[1;32m     70\u001b[0m                         \u001b[0mvInputs_ids_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvAttention_a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     )[0]  # all token embeddings A\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         return self.transformer(\n\u001b[0m\u001b[1;32m    570\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    348\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             )\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0msa_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msa_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0msa_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# Feed Forward Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    190\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2484\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         )\n\u001b[0;32m-> 2486\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 1; 10.92 GiB total capacity; 7.16 GiB already allocated; 4.38 MiB free; 7.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainDict = {}\n",
    "# increase from 1 epoch if need be \n",
    "for epoch in range(1):\n",
    "    \n",
    "    model.train()  # make sure model is in training mode\n",
    "    \n",
    "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
    "    loop = tqdm(trainLoader, leave=True)\n",
    "    \n",
    "    for i, batch in enumerate(loop):\n",
    "        if i < 41: \n",
    "            # zero all gradients on each new step\n",
    "            optim.zero_grad()\n",
    "\n",
    "            # prepare batches and more all to the active device\n",
    "            inputs_ids_a = batch['text1_input_ids'].to(device)\n",
    "            inputs_ids_b = batch['text2_input_ids'].to(device)\n",
    "            attention_a = batch['text1_attention_mask'].to(device)\n",
    "            attention_b = batch['text2_attention_mask'].to(device)\n",
    "\n",
    "            #ground truth labels need to go to device as well \n",
    "            label = batch['ground_truth'].to(device)\n",
    "\n",
    "            # extract token embeddings from BERT\n",
    "            u = model(\n",
    "                inputs_ids_a, attention_mask=attention_a\n",
    "            )[0]  # all token embeddings A\n",
    "\n",
    "            v = model(\n",
    "                inputs_ids_b, attention_mask=attention_b\n",
    "            )[0]  # all token embeddings B\n",
    "\n",
    "            # get the mean pooled vectors\n",
    "            u = mean_pool(u, attention_a)\n",
    "            v = mean_pool(v, attention_b)\n",
    "\n",
    "            # build the |u-v| tensor\n",
    "            uv = torch.sub(u, v)\n",
    "            uv_abs = torch.abs(uv)\n",
    "\n",
    "            # concatenate u, v, |u-v|\n",
    "            x = torch.cat([u, v, uv_abs], dim=-1)\n",
    "\n",
    "            # process concatenated tensor through FFNN\n",
    "            x = ffnn(x)\n",
    "\n",
    "            # calculate the 'MSE-loss' between predicted and true label\n",
    "            loss = loss_func(x, label)\n",
    "\n",
    "            \n",
    "            #get validation loss\n",
    "            model.eval()\n",
    "            vLossList = []\n",
    "            vCorrList = []\n",
    "            vPred = []\n",
    "            vGT = []\n",
    "            if i % 20 == 0: \n",
    "                for vBatch in validLoader: \n",
    "                    # prepare batches and more all to the active device\n",
    "                    vInputs_ids_a = vBatch['text1_input_ids'].to(device)\n",
    "                    vInputs_ids_b = vBatch['text2_input_ids'].to(device)\n",
    "                    vAttention_a = vBatch['text1_attention_mask'].to(device)\n",
    "                    vAttention_b = vBatch['text2_attention_mask'].to(device)\n",
    "\n",
    "                    #ground truth labels need to go to device as well \n",
    "                    vLabel = vBatch['ground_truth'].to(device)\n",
    "\n",
    "                    # extract token embeddings from BERT\n",
    "                    u = model(\n",
    "                        vInputs_ids_a, attention_mask=vAttention_a\n",
    "                    )[0]  # all token embeddings A\n",
    "\n",
    "                    v = model(\n",
    "                        vInputs_ids_b, attention_mask=vAttention_b\n",
    "                    )[0]  # all token embeddings B\n",
    "\n",
    "                    # get the mean pooled vectors\n",
    "                    u = mean_pool(u, vAttention_a)\n",
    "                    v = mean_pool(v, vAttention_b)\n",
    "\n",
    "                    # build the |u-v| tensor\n",
    "                    uv = torch.sub(u, v)\n",
    "                    uv_abs = torch.abs(uv)\n",
    "\n",
    "                    # concatenate u, v, |u-v|\n",
    "                    valX = torch.cat([u, v, uv_abs], dim=-1)\n",
    "\n",
    "                    # process concatenated tensor through FFNN\n",
    "                    valX = ffnn(valX)\n",
    "\n",
    "                    vLoss = loss_func(valX, vLabel)\n",
    "                    vLossList.append(vLoss.item())\n",
    "                    vPred.append(valX)\n",
    "                    vGT.append(vLabel)\n",
    "        \n",
    "             #get training metrics \n",
    "            trainDict[i] = {\"trainLoss\": loss.item(), \"testLoss\":np.mean(vLossList)} \n",
    "            \n",
    "            \n",
    "            # using loss, calculate gradients and then optimize\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            # update learning rate scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            # update the TDQM progress bar\n",
    "            loop.set_description(f'Epoch {epoch}')\n",
    "            loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 0.4697167239540121\n"
     ]
    }
   ],
   "source": [
    "#check memory \n",
    "t = torch.cuda.mem_get_info()\n",
    "\n",
    "used = torch.cuda.memory_allocated(device=\"cuda:4\")\n",
    "#proportion of free memory \n",
    "#print(\"used: \" used / t[1])\n",
    "print(\"used: \" + str(t[0]/t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf = pd.DataFrame.from_dict(trainDict, orient=\"index\")\n",
    "trainDf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=[14,4])\n",
    "axs[0].plot(trainDf[\"trainLoss\"])\n",
    "axs[0].set_title(\"Train Loss\", size=14)\n",
    "axs[0].set_ylabel(\"MSE Loss\",size=14)\n",
    "axs[0].set_xlabel(\"batch\",size=14)\n",
    "\n",
    "axs[1].plot(trainDf.dropna()[\"testLoss\"])\n",
    "axs[1].set_title(\"Validation Loss\",size=14)\n",
    "axs[1].set_ylabel(\"MSE Loss\",size=14)\n",
    "axs[1].set_xlabel(\"batch\",size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(label)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func(x, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainDf[\"testLoss\"].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
